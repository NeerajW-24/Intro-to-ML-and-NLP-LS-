# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vEBddv7tsOMhaFAIB2-C0GGTsLmRMagz
"""

!pip install "fsspec==2023.6.0"

!pip install transformers datasets evaluate --quiet

import torch
import math
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    GPT2LMHeadModel,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer,
)

tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")

def tokenize(example):
    return tokenizer(example["text"])

tokenized = dataset.map(tokenize, batched=True, remove_columns=["text"])

block_size = 128

def group_texts(examples):
    joined = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = (len(joined["input_ids"]) // block_size) * block_size
    result = {
        k: [t[i:i + block_size] for i in range(0, total_length, block_size)]
        for k, t in joined.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result

lm_data = tokenized.map(group_texts, batched=True)

train_data = lm_data["train"].select(range(8000))
eval_data = lm_data["validation"].select(range(1000))

model = GPT2LMHeadModel.from_pretrained("gpt2")
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

args = TrainingArguments(
    output_dir="/tmp",
    eval_strategy="epoch",
    learning_rate=3e-5,
    weight_decay=0.01,
    num_train_epochs=3,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    save_strategy="no",
    logging_steps=10,
    push_to_hub=False,
    report_to="none",
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_data,
    eval_dataset=eval_data,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

def top_k_accuracy(model, dataset, k=5, n=100):
    model.eval()
    model.to("cuda" if torch.cuda.is_available() else "cpu")
    correct, total = 0, 0

    for d in dataset.select(range(n)):
        ids = torch.tensor(d["input_ids"]).unsqueeze(0)
        labels = torch.tensor(d["labels"]).unsqueeze(0)
        ids, labels = ids.to(model.device), labels.to(model.device)

        with torch.no_grad():
            logits = model(ids).logits[:, :-1, :]
            topk = logits.topk(k, dim=-1).indices
            match = (topk == labels[:, 1:].unsqueeze(-1)).any(-1)
            correct += match.sum().item()
            total += match.numel()

    return correct / total

topk = top_k_accuracy(model, eval_data, k=5, n=100)
print(f"Top-5 Accuracy: {topk * 100:.2f}%")

result = trainer.evaluate()
perplexity = math.exp(result["eval_loss"])
print(f"Perplexity: {perplexity:.2f}")

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

prompt = "The indian ocean"
inputs = tokenizer(prompt, return_tensors="pt").to(device)

output = model.generate(
    **inputs,
    max_new_tokens=10,
    do_sample=True,
    top_k=50,
    top_p=0.95,
    temperature=0.5,
    pad_token_id=tokenizer.eos_token_id
)

generated = tokenizer.decode(output[0], skip_special_tokens=True)
print("Prompt:", prompt)
print("Generated text:", generated)







